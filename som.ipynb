{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Organizing Map\n",
    "Implementation of a Self Organizing Map (SOM) in python. This program and documentation was written with the help of the AI tool [Github Copilot](https://github.com/features/copilot).\n",
    "\n",
    "The SOM is a type of artificial neural network that is trained using unsupervised learning to produce a low-dimensional representation of the input space of the training samples. The model typically consists of a two-dimensional grid of nodes, where each node is associated with a weight vector of the same dimension as the input vectors. The nodes are arranged in a hexagonal or rectangular grid, and are usually initialized randomly. The training consists of two main steps: competition and cooperation. In the competition step, the node with the weight vector that is most similar to the input vector is selected as the winner. In the cooperation step, the weights of the winning node and its neighbors are updated to move closer to the input vector. The training is repeated for a number of epochs, and the weights of the nodes converge to represent the input space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model parameters\n",
    "The program is setup up to allow easy configuration. The following options can be adjusted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "RATIO = 0.7 # ratio with which the data is split into training and testing data\n",
    "MAP_WIDTH = 10 # width of the SOM map\n",
    "MAP_HEIGHT = 10 # height of the SOM map\n",
    "ITERATIONS = 10000 # number of iterations\n",
    "\n",
    "NEIGHBORHOOD_FUNCTION = 'bubble' # neighborhood function, can be 'gaussian' or 'bubble'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and pre-process the dataset\n",
    "Dataset is retrieved from a file and pre-processed.  \n",
    "To allow for better visualization of the results, a two-dimensional dataset is used. It is an artificial dataset with two features and three classes, all distinctly gropued into three clusters, with minor outliers in each class. The dataset is generated using a [open-source cluster painting tool](https://www.joonas.io/cluster-paint/).\n",
    "1. Load the dataset from a file, separate the features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'datasets/colors.csv' # path to the dataset file\n",
    "\n",
    "def load_dataset(dataset_path):\n",
    "    with open(dataset_path) as f:\n",
    "        inputs = []\n",
    "        labels = []\n",
    "        \n",
    "        # read the input file line by line\n",
    "        for line in f:\n",
    "            split_line = line.split(',')\n",
    "            \n",
    "            n_features = len(split_line) - 1\n",
    "            \n",
    "            # save the first n-1 values to an entry\n",
    "            input_entry = [float(split_line[i]) for i in range(0, n_features)]\n",
    "            inputs.append(input_entry)\n",
    "            \n",
    "            # save last value to labels\n",
    "            labels.append(float(split_line[-1]))\n",
    "            \n",
    "    return inputs, labels, n_features, len(set(labels))\n",
    "\n",
    "inputs, labels, n_features, n_classes = load_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Apply normalization to the features to scale them to the range [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def normalize_inputs(inputs):\n",
    "    # Create an instance of MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Fit the scaler to the features and transform the features\n",
    "    return scaler.fit_transform(inputs)\n",
    "\n",
    "inputs = np.array(normalize_inputs(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the dataset only has two freature, we can review it by plotting the different classes in separate graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('Data distribution')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.scatter(inputs[:,0], inputs[:,1], c=labels, cmap='viridis')\n",
    "plt.show()\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,5))\n",
    "\n",
    "class_1 = [inputs[i] for i in range(len(inputs)) if labels[i] == 0]\n",
    "ax1.set_title('Class 1')\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.scatter([x[0] for x in class_1], [x[1] for x in class_1], color='red')\n",
    "\n",
    "class_2 = [inputs[i] for i in range(len(inputs)) if labels[i] == 1]\n",
    "ax2.set_title('Class 2')\n",
    "ax2.set_xlim([0, 1])\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.scatter([x[0] for x in class_2], [x[1] for x in class_2], color='g')\n",
    "\n",
    "class_3 = [inputs[i] for i in range(len(inputs)) if labels[i] == 2]\n",
    "ax3.set_title('Class 3')\n",
    "ax3.set_xlim([0, 1])\n",
    "ax3.set_ylim([0, 1])\n",
    "ax3.scatter([x[0] for x in class_3], [x[1] for x in class_3], color='blue')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Shuffle the dataset and split it into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "    \n",
    "def shuffle_data(inputs, labels):\n",
    "    zipped = list(zip(inputs, labels))\n",
    "    shuffle(zipped)\n",
    "    inputs, labels = zip(*zipped)\n",
    "    \n",
    "    return np.array(inputs), np.array(labels)\n",
    "    \n",
    "def split_data(inputs, labels, ratio):\n",
    "    # split lists into training and validation data\n",
    "    # convert lists to numpy arrays\n",
    "    ratio_index = int(len(inputs)*ratio)\n",
    "    train_inputs = np.array(inputs[:ratio_index])\n",
    "    train_labels = np.array(labels[:ratio_index])\n",
    "    val_inputs = np.array(inputs[ratio_index:])\n",
    "    val_labels = np.array(labels[ratio_index:])\n",
    "    \n",
    "    return train_inputs, train_labels, val_inputs, val_labels\n",
    "    \n",
    "inputs, labels = shuffle_data(inputs, labels)\n",
    "\n",
    "train_inputs, train_labels, val_inputs, val_labels = split_data(inputs, labels, RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training data length: {len(train_inputs)}')\n",
    "print(f'Validation data length: {len(val_inputs)}')\n",
    "print(f'Data sample: \\n\\t{train_inputs[0]} => {train_labels[0]}')\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "\n",
    "ax1.set_title('Training data')\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.scatter(train_inputs[:, 0], train_inputs[:, 1], c=train_labels)\n",
    "\n",
    "ax2.set_title('Validation data')\n",
    "ax2.set_xlim([0, 1])\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.scatter(val_inputs[:, 0], val_inputs[:, 1], c=val_labels)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions\n",
    "Several helper functions are defined for the more complex arithmetic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(a, b):\n",
    "    \"\"\"\n",
    "    Calculates the Euclidean distance between two vectors.\n",
    "\n",
    "    Parameters:\n",
    "    a (numpy.ndarray): The first vector.\n",
    "    b (numpy.ndarray): The second vector.\n",
    "\n",
    "    Returns:\n",
    "    float: The Euclidean distance between the two vectors.\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_distance(a, b):\n",
    "    \"\"\"\n",
    "    Calculates the Manhattan distance between two points in a multi-dimensional space.\n",
    "\n",
    "    Parameters:    Calculates the Manhattan distance between two points.\n",
    "\n",
    "    Parameters:\n",
    "    a (tuple): The coordinates of the first point in the form (x, y).\n",
    "    b (tuple): The coordinates of the second point in the form (x, y).\n",
    "\n",
    "    Returns:\n",
    "    int: The Manhattan distance between the two points.\n",
    "    \"\"\"\n",
    "    return max(abs(a[0] - b[0]), abs(a[1] - b[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate(iteration, max_iterations):\n",
    "    \"\"\"\n",
    "    Calculates the learning rate for the current iteration.\n",
    "\n",
    "    Parameters:\n",
    "    iteration (int): The current iteration.\n",
    "    max_iterations (int): The total number of iterations.\n",
    "\n",
    "    Returns:\n",
    "    float: The learning rate for the current iteration.\n",
    "    \"\"\"\n",
    "    return (1 - iteration / max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_neighborhood(learning_rate, radius, distance):\n",
    "    \"\"\"\n",
    "    Calculates the Gaussian neighborhood function.\n",
    "\n",
    "    Parameters:\n",
    "    learning_rate (float): The current learning rate.\n",
    "    radius (float): The current radius of the neighborhood.\n",
    "    distance (float): The distance between the neuron and the BMU.\n",
    "\n",
    "    Returns:\n",
    "    float: The Gaussian neighborhood value.\n",
    "    \"\"\"\n",
    "    if radius == 0:\n",
    "        return 1.0\n",
    "    return learning_rate * np.exp(-distance / (2 * (radius ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "def bubble_neighborhood(learning_rate, radius, max_radius):\n",
    "    \"\"\"\n",
    "    Calculates the Bubble neighborhood function.\n",
    "\n",
    "    Parameters:\n",
    "    learning_rate (float): The current learning rate.\n",
    "    max_radius (float): The maximum radius of the neighborhood.\n",
    "\n",
    "    Returns:\n",
    "    float: The Bubble neighborhood value.\n",
    "    \"\"\"\n",
    "    return learning_rate if radius <= ceil(learning_rate * max_radius) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the SOM class\n",
    "The SOM class contains the main methods for training the model and predicting the class labels of the samples. The class is initialized with the model parameters and the dataset, and contains the following methods:\n",
    "\n",
    "1. `train()`: Trains the model using the training set.\n",
    "\n",
    "2. `predict()`: Predicts the class labels of the samples in the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class SOM:\n",
    "    # Initialize the SOM with random neruon weights\n",
    "    def __init__(self, map_width, map_height, input_dim):\n",
    "        self._map_width = map_width\n",
    "        self._map_height = map_height\n",
    "        self._neurons = np.random.rand(map_width, map_height, input_dim)\n",
    "        \n",
    "    def _find_winning_neuron(self, input):\n",
    "        winning_neuron = None\n",
    "        best_distance = float('inf')\n",
    "        \n",
    "        for i in range(self._map_width):\n",
    "            for j in range(self._map_height):\n",
    "                distance = euclidean_distance(input, self._neurons[i][j])\n",
    "                if distance < best_distance:\n",
    "                    best_distance = distance\n",
    "                    winning_neuron = (i, j)\n",
    "        \n",
    "        return winning_neuron\n",
    "    \n",
    "    def _find_second_best_neuron(self, input):\n",
    "        best_distance = float('inf')\n",
    "        second_best_distance = float('inf')\n",
    "        winning_neuron = None\n",
    "        second_best_neuron = None\n",
    "        \n",
    "        for i in range(self._map_width):\n",
    "            for j in range(self._map_height):\n",
    "                distance = euclidean_distance(input, self._neurons[i][j])\n",
    "                if distance < best_distance:\n",
    "                    second_best_distance = best_distance\n",
    "                    best_distance = distance\n",
    "                    second_best_neuron = winning_neuron\n",
    "                    winning_neuron = (i, j)\n",
    "                elif distance < second_best_distance:\n",
    "                    second_best_distance = distance\n",
    "                    second_best_neuron = (i, j)\n",
    "        \n",
    "        return second_best_neuron\n",
    "    \n",
    "    def _update_som(self, input, winner, curr_iteration, max_iterations):\n",
    "        for i in range(self._map_width):\n",
    "            for j in range(self._map_height):                \n",
    "                neighborhood = 0\n",
    "                if NEIGHBORHOOD_FUNCTION == 'gaussian':\n",
    "                    neighborhood = gaussian_neighborhood(\n",
    "                        learning_rate(curr_iteration, max_iterations),\n",
    "                        manhattan_distance(winner, (i, j)), \n",
    "                        euclidean_distance(input, self._neurons[i][j]))\n",
    "                elif NEIGHBORHOOD_FUNCTION == 'bubble':\n",
    "                    neighborhood = bubble_neighborhood(\n",
    "                        learning_rate(curr_iteration, max_iterations),\n",
    "                        manhattan_distance(winner, (i, j)), \n",
    "                        max(self._map_width, self._map_height))\n",
    "                else:\n",
    "                    raise ValueError('Invalid neighborhood function')\n",
    "                \n",
    "                delta = neighborhood * (input - self._neurons[i][j])\n",
    "                self._neurons[i][j] += delta\n",
    "              \n",
    "    def _run_iteration(self, inputs, curr_iteration, max_iterations):\n",
    "        for input in inputs:\n",
    "            winner = self._find_winning_neuron(input)\n",
    "            self._update_som(input, winner, curr_iteration, max_iterations) \n",
    "                         \n",
    "    def train(self, inputs, labels, max_iterations):\n",
    "        \"\"\"\n",
    "        Trains the self-organizing map (SOM) with the given inputs for the specified number of iterations.\n",
    "\n",
    "        Parameters:\n",
    "        - inputs (numpy.ndarray): The input data to train the SOM.\n",
    "        - max_iterations (int): The number of iterations to train the SOM.\n",
    "        \"\"\"       \n",
    "        quantization_errors = []\n",
    "        topographic_errors = []\n",
    "        \n",
    "        for i in range(max_iterations):\n",
    "            q_error, t_error = self._calculate_errors(inputs)\n",
    "            quantization_errors.append(q_error)\n",
    "            topographic_errors.append(t_error)\n",
    "            \n",
    "            if (i + 1) % (max_iterations / 10) == 0:\n",
    "                print(f'Iteration {i + 1}/{max_iterations}')\n",
    "                print(f'Quantization error: {q_error}')\n",
    "                print(f'Topographic error: {t_error}')\n",
    "            \n",
    "            self._run_iteration(inputs, i, max_iterations)\n",
    "            \n",
    "        # Plot the quantization and topographic errors\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "        \n",
    "        ax1.set_title('Quantization Error')\n",
    "        ax1.plot(quantization_errors)\n",
    "        \n",
    "        ax2.set_title('Topographic Error')\n",
    "        ax2.plot(topographic_errors)\n",
    "        \n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot the data distribution and the SOM neuron weights\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "        \n",
    "        ax1.set_title('Data distribution')\n",
    "        ax1.set_xlim([0, 1])\n",
    "        ax1.set_ylim([0, 1])\n",
    "        ax1.scatter(inputs[:,0], inputs[:,1], c=labels, cmap='viridis')\n",
    "        \n",
    "        weights = []\n",
    "        for i in range(self._map_width):\n",
    "            for j in range(self._map_height):\n",
    "                neuron = self._neurons[i][j]\n",
    "                weights.append(list(neuron))\n",
    "        \n",
    "        weights = np.array(weights)\n",
    "        \n",
    "        ax2.set_title('SOM Neuron Weights')\n",
    "        ax2.set_xlim([0, 1])\n",
    "        ax2.set_ylim([0, 1])\n",
    "        ax2.scatter(weights[:, 0], weights[:, 1])\n",
    "        \n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def _calculate_errors(self, inputs):\n",
    "        q_error = 0\n",
    "        t_error = 0\n",
    "        \n",
    "        for input in inputs:\n",
    "            winner = self._find_winning_neuron(input)\n",
    "            second_best = self._find_second_best_neuron(input)\n",
    "            \n",
    "            q_error += euclidean_distance(input, self._neurons[winner[0]][winner[1]])\n",
    "            \n",
    "            if manhattan_distance(winner, second_best) > 1:\n",
    "                t_error += 1\n",
    "            \n",
    "        return q_error / len(inputs), t_error / len(inputs)\n",
    "        \n",
    "    def predict(self, inputs, labels):\n",
    "        prediction_table = np.zeros((self._map_width, self._map_height, n_classes))\n",
    "        \n",
    "        # For each input, find the winning neuron and increment the counter for the corresponding label\n",
    "        for i, input in enumerate(inputs):\n",
    "            winner = self._find_winning_neuron(input)\n",
    "            prediction_table[winner[0]][winner[1]][int(labels[i])] += 1\n",
    "        \n",
    "        fig, axs = plt.subplots(1, n_classes, figsize=(5 * n_classes,5))\n",
    "        \n",
    "        for i in range(n_classes):\n",
    "            values = prediction_table[:, :, i]\n",
    "            values = values / np.max(values)\n",
    "            values.reshape((self._map_width, self._map_height))\n",
    "            \n",
    "            axs[i].set_title(f'Class {i}')\n",
    "            axs[i].imshow(values, cmap='Purples', interpolation='nearest')\n",
    "            \n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        self._plot_u_matrix(prediction_table)\n",
    "    \n",
    "    def _plot_u_matrix(self, prediction_table):\n",
    "        u_matrix = np.zeros((self._map_width, self._map_height))\n",
    "        \n",
    "        # List of all possible directions to neighboring neurons\n",
    "        neighbor_directions = [(0, 1), (1, 0), (0, -1), (-1, 0), (1, 1), (1, -1), (-1, 1), (-1, -1)]\n",
    "        \n",
    "        for i in range(self._map_width):\n",
    "            for j in range(self._map_height):\n",
    "                for direction in neighbor_directions:\n",
    "                    if 0 <= i + direction[0] < self._map_width and 0 <= j + direction[1] < self._map_height:\n",
    "                        u_matrix[i][j] += euclidean_distance(self._neurons[i][j], self._neurons[i + direction[0]][j + direction[1]])\n",
    "                u_matrix[i][j] /= len(neighbor_directions)\n",
    "        \n",
    "        plt.title('U-Matrix')\n",
    "        plt.imshow(u_matrix, cmap='Greys', interpolation='nearest')\n",
    "        \n",
    "        for i in range(self._map_width):\n",
    "            for j in range(self._map_height):\n",
    "                plt.text(j, i, prediction_table[i][j].argmax(),\n",
    "                    ha=\"center\", va=\"center\", color=\"w\")\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create and train the SOM model. The epoch count is configurable. Quanitization error is calculated after each epoch to evaluate the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som = SOM(MAP_WIDTH, MAP_HEIGHT, len(train_inputs[0]))\n",
    "som.train(train_inputs, train_labels, ITERATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we pass in new, validation, data to the model and plot the results. For each input entry, we find the best matching unit (BMU), then increase the counter for the BMU's class. Finally, we plot the results in three separate graphs, one for each class. Additionally, we plot the U-matrix, which is a visualization of the distance between the nodes in the SOM grid. It helps to visualize the topology of the input space and the relationships between the nodes. The U-matrix is labeled with the class labels of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som.predict(val_inputs, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with different dataset\n",
    "To test the functionality of the SOM, the same process is repeated with a different dataset.\n",
    "In this example, the dataset is the [Iris dataset](https://archive.ics.uci.edu/dataset/53/iris), which is a popular dataset for testing machine learning algorithms. It contains 150 samples of iris flowers, each with 4 features: sepal length, sepal width, petal length, and petal width. The goal is to cluster the samples into 3 classes based on the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels, n_features, n_classes = load_dataset('datasets/iris.csv')\n",
    "inputs, labels = shuffle_data(np.array(normalize_inputs(inputs)), labels)\n",
    "train_inputs, train_labels, val_inputs, val_labels = split_data(inputs, labels, RATIO)\n",
    "\n",
    "som = SOM(MAP_WIDTH, MAP_HEIGHT, len(train_inputs[0]))\n",
    "som.train(train_inputs, train_labels, ITERATIONS)\n",
    "som.predict(val_inputs, val_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
